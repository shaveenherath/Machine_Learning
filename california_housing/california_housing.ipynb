{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDRIguC9jyWm"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNUUuLcKjyWn"
      },
      "source": [
        "Author: Sampath K Perera and  Kajhanan Kailainathan\n",
        "\n",
        "This notebook is based on [this](https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html) scikit learn tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5-VH5HSjyWn"
      },
      "source": [
        "## Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3TYUZg7hLbh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn import model_selection as  model_selection\n",
        "#load the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlYbAhXKjyWo"
      },
      "source": [
        "## Visualize the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RYaOx8GjyWo"
      },
      "source": [
        "Fetch the California Housing Dataset through the *datasets* module of the *sklearn* library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T6KjxArjyWo"
      },
      "outputs": [],
      "source": [
        "california_housing = fetch_california_housing(as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmleNIfijyWo"
      },
      "outputs": [],
      "source": [
        "type(california_housing.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SSxGSvBjyWp"
      },
      "source": [
        "The entire tabular dataset could be accessed using the .data attribute. The .head() method is used to visualize a compact dataframe of only 5 entries. This can be used to visualize the independent variable values and their names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DwbEew4j-qp"
      },
      "outputs": [],
      "source": [
        "california_housing.data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUq9OzNnjyWp"
      },
      "source": [
        "The depend variable name and values could be accessed through the .target attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_J4xT3vi1yb"
      },
      "outputs": [],
      "source": [
        "# print dependent variable (y)\n",
        "california_housing.target.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLss3oXhjyWp"
      },
      "source": [
        "The .frame attribute contains a Dataframe of both the dependent and independent variables. The .info() method gives the Column name, Non-Null count (number of entries with non-null values), and Data Type. Verify by running the code snippet below. Do try out the .info() method on .target and .data attributes as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v06P__Dk6AR"
      },
      "outputs": [],
      "source": [
        "california_housing.frame.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSGWAQ4cjyWp"
      },
      "source": [
        "The .hist() method can be called upon a dataframe to plot histogram distribution for each column variables. This can be used to get an idea of the distribution of each of the variables in an independent manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exrLEEHQlIqk"
      },
      "outputs": [],
      "source": [
        "california_housing.frame.hist(figsize=(12, 10), bins=30, edgecolor=\"black\")\n",
        "plt.subplots_adjust(hspace=0.7, wspace=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDAlH8ZFjyWp"
      },
      "source": [
        "## Data Analysis (Checking for Outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHH3hWAwjyWp"
      },
      "source": [
        "From the above variables, the histogram plots of the 'Average Rooms', 'Average Bedrooms', 'Average Population', and 'Population' seem to have a significant skew. (i.e), the data is bulged within a short range of the entire range of the data distribution.\n",
        "\n",
        "This points at a potential source of outliers.\n",
        "\n",
        "The .describe() method can be called upon the selected features, which gives out the mean, standard deviation, inter quartile range values, and other significant stastical measures which can be used to analyse the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hi0kYQNmxr1"
      },
      "outputs": [],
      "source": [
        "features_of_interest = [\"AveRooms\", \"AveBedrms\", \"AveOccup\", \"Population\"]\n",
        "california_housing.frame[features_of_interest].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlfG8kvtjyWp"
      },
      "source": [
        "A large gap on the 75th percentile value and the maximum value indicates possible outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjnIbOcfjyWq"
      },
      "source": [
        "## Linear Regression on a Single Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaacilpnPsP"
      },
      "source": [
        "Let us perform linear regression on a single variable with potential outliers. We will compare different methods and their performance at the presence of outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huhR5CZdnkQ6"
      },
      "outputs": [],
      "source": [
        "features_of_interest = [\"AveRooms\"]\n",
        "california_housing.frame[features_of_interest].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZVsLdLon7LD"
      },
      "outputs": [],
      "source": [
        "# Load the  dataset\n",
        "data_X, data_y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# select the \"AveRooms\" data\n",
        "data_X=data_X[:,2]\n",
        "\n",
        "#save index of the maximum value, will be used later\n",
        "index_max=np.argmax(data_X)\n",
        "max_x=data_X[index_max]\n",
        "max_y=data_y[index_max]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUHV1JGIpuhP"
      },
      "outputs": [],
      "source": [
        "#verification that we have selected correct set\n",
        "print(np.max(data_X),np.min(data_X),np.mean(data_X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9pQVa5qjyWq"
      },
      "source": [
        "Select only 100 datapoints at random and perform regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0omiS6emqwJ8"
      },
      "outputs": [],
      "source": [
        "#we have lot of data samples, just select 100 of them\n",
        "data_X=data_X[100:200]\n",
        "data_y=data_y[100:200]\n",
        "#remember index starts from 0\n",
        "# Split the data into training/testing sets\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(data_X, data_y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val  = model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"Total dataset elements\",data_X.shape)\n",
        "print(\"Train dataset elements\",X_train.shape)\n",
        "print(\"Validatiom dataset elements\",X_val.shape)\n",
        "print(\"Test dataset elements\",X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb-GSFsdjyWq"
      },
      "source": [
        "Most sklearn methods (and also methods from other machine learning libraries), deal with tensors. The above shape (100,) for instance refers to a numpy array of 100 scalar values. However, we must convert this to a numpy array of 100 1 dimensional tensors.\n",
        "\n",
        "**This intermediate step will be commonly encountered even when working with other Machine Learning libraries, thus is helpful to make a note on this if you are already not aware**\n",
        "\n",
        "It is encouraged that you visualize the shapes of the X_train and other variables before and after applying the .reshape() method using the .shape attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EH4V2PKv1Z3"
      },
      "outputs": [],
      "source": [
        "#reshape data as vectors\n",
        "# print(Shape before reshaping)\n",
        "# print(X_train.shape)\n",
        "X_train = X_train.reshape(X_train.shape[0], 1)\n",
        "# print(Shape after reshaping)\n",
        "# print(X_train.shape)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 1)\n",
        "y_train = y_train.reshape(y_train.shape[0], 1)\n",
        "y_test = y_test.reshape(y_test.shape[0], 1)\n",
        "yval = y_val.reshape(y_val.shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttHTKWepjyWq"
      },
      "source": [
        "The *linear_model.LinearRegression()* module from the *sklearn* library can be used to perform linear regression and tabulate the corresponding variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjKFFZSkuT9l"
      },
      "outputs": [],
      "source": [
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# The intercept\n",
        "print(\"Intercept w_0: \\n\", regr.intercept_)\n",
        "# The coefficients\n",
        "print(\"Coefficient w_1: \\n\", regr.coef_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6psDa7WjyWq"
      },
      "source": [
        "Instead of using the *linear_model.LinearRegression()* module, the known equation $\\hat{\\beta} = (X^T X)^{-1}X^T y$ can be directly applied using numpy methods.\n",
        "\n",
        "Verify that both the answers are the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuvUbFZCRkgi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Reshape y_train to ensure it's a column vector\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "# Add a column of 1s to X_train for the intercept term\n",
        "intercept = np.ones((X_train.shape[0], 1))\n",
        "X_train_with_intercept = np.hstack((intercept, X_train))\n",
        "\n",
        "# Calculate beta_hat using the normal equation\n",
        "X_transpose = X_train_with_intercept.T\n",
        "X_transpose_X = np.dot(X_transpose, X_train_with_intercept)\n",
        "X_transpose_X_inv = np.linalg.inv(X_transpose_X)\n",
        "X_transpose_y = np.dot(X_transpose, y_train)\n",
        "beta_hat = np.dot(X_transpose_X_inv, X_transpose_y)\n",
        "\n",
        "# Extract the intercept and coefficients\n",
        "intercept_hat = beta_hat[0]\n",
        "coefficients = beta_hat[1:]\n",
        "\n",
        "print(\"Intercept w_0 :\", intercept_hat)\n",
        "print(\"Coefficient w_1:\", coefficients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycB_ZvFijyWq"
      },
      "source": [
        "The variables RSS (Residual Sum of Squares), RSE (Residual Standard Error), and TSS (Total Sum of Squares) can be tabulated as shown below by directly applying numpy methods. The predicted values are obtained using the .predict() method from the instantiation of linear_model.LinearRegression()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqTuNpL7OnrK"
      },
      "outputs": [],
      "source": [
        "yhat=regr.predict(X_train)\n",
        "\n",
        "# Residual Sum of Sqaures (RSS)\n",
        "RSS = np.sum((yhat - y_train)**2)\n",
        "print('RSS=', RSS)\n",
        "\n",
        "N=len(y_train)\n",
        "print('Number of Datapoints=',N)\n",
        "\n",
        "# Residual Standard Error (RSE)\n",
        "RSE = np.sqrt(1/(N-2)*RSS)\n",
        "print('RSE=', RSE)\n",
        "\n",
        "# Total Sum of Squares (TSS)\n",
        "TSS = np.sum((y_train- np.mean(y_train))**2)\n",
        "print('TSS=', TSS)\n",
        "\n",
        "R2 = (TSS - RSS)/TSS\n",
        "print('R2 (from direct calculations)=', R2)\n",
        "\n",
        "# Calculation of R2 using sklearn\n",
        "R2 = regr.score(X_train,y_train)\n",
        "\n",
        "print('R2 (from sklearn module)=', R2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEduifT7jyWr"
      },
      "source": [
        "Calculate the t-statistic and p-values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJwFNMINOnrK"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import t\n",
        "\n",
        "\n",
        "w_1=regr.coef_\n",
        "w_0=regr.intercept_\n",
        "sigma2 = np.var(y_train - yhat)\n",
        "SE2w0 =  sigma2*(1/N + np.mean(X_train)**2/np.sum((X_train - np.mean(X_train))**2))\n",
        "SE2w1 = sigma2/np.sum((X_train - np.mean(X_train))**2)\n",
        "tw1 = (w_1 - 0)/np.sqrt(SE2w1)\n",
        "tw0 = (w_0 - 0)/np.sqrt(SE2w0)\n",
        "\n",
        "print('Standard errors for intecept and w1: ', SE2w0, SE2w1)\n",
        "print('t-statistic for intecept and w1: ', tw0, tw1)\n",
        "pw1 = t.sf(np.abs(tw1), N-2)\n",
        "\n",
        "pw0 = t.sf(np.abs(tw0), N-2)\n",
        "print('p-value  for intecept and w1: ', pw0, pw1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYqN5btYjyWr"
      },
      "source": [
        "Use the calculated regression coefficients, and check its validity,\n",
        "1. Quantitatively : Using Mean squared error and Coefficient of Determination\n",
        "2. Qualitatively : By plotting and visualizing the test datapoints as a scatter plot against the regression line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5jsKUND4USP"
      },
      "outputs": [],
      "source": [
        "# Make predictions using the testing set\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test, y_test, color=\"black\", label='Data points')\n",
        "plt.plot(X_test, y_pred, color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Testing data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cas70MBMjyWy"
      },
      "source": [
        "The coeffeicient of determination value is well above 0.5. This means that the regression method has worked to some length. However, we should also consider the fact that outliers could have been removed when we took a random sample of the training set to determine the regression coefficients.\n",
        "\n",
        "Let us observe how the presence of outliers impacts the regression method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIQiBAlqjyWy"
      },
      "source": [
        "## Regression with Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT0EVv1mjyWy"
      },
      "source": [
        "Perform regression after including the maximum x and corresponding y value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPMmrNwB3Kr3"
      },
      "outputs": [],
      "source": [
        "# load maximum x value and corresponding y value to the training dataset\n",
        "X_train[59,:]=max_x\n",
        "y_train[59,:]=max_y\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "print(\"intercept: \\n\", regr.intercept_)\n",
        "\n",
        "#see how training line look like\n",
        "\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_train, y_train, color=\"black\", )\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2)\n",
        "\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.title(\"Training data\")\n",
        "plt.show()\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Training data (zoom)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "# The intercept\n",
        "print(\"Coefficients: \\n\", regr.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3umA86LjyWy"
      },
      "source": [
        "See the predictions using test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcu-XhZR6QYc"
      },
      "outputs": [],
      "source": [
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test, y_test, color=\"black\", label='Data points')\n",
        "plt.plot(X_test, y_pred, color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Testing data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDuUkm2hjyWy"
      },
      "source": [
        "## RANSAC (RANdom SAmple Consensus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBJuSq0GjyWz"
      },
      "source": [
        "RANSAC is an iterative parameter estimation method that performs well even at the presence of outliers. Click [this](http://www.cse.yorku.ca/~kosta/CompVis_Notes/ransac.pdf) to learn more about RANSAC.\n",
        "\n",
        "*linear_model.RANSACRegressor()* from *sklearn* is used to implement the RANSAC algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ1gZdtvByub"
      },
      "outputs": [],
      "source": [
        "# Train the model using the training sets\n",
        "ransac = linear_model.RANSACRegressor()\n",
        "#The RANSAC regressor automatically splits the data into inliers and outliers, and the fitted line is determined only by the identified inliers.\n",
        "\n",
        "ransac.fit(X_train, y_train)\n",
        "\n",
        "# Plot outputs\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "plt.plot(X_train, ransac.predict(X_train), color=\"red\", linewidth=2, label=r'RANSAC Regressor')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.title(\"Training data\")\n",
        "plt.show()\n",
        "plt.subplots(1, 1)\n",
        "plt.scatter(X_train, y_train, color=\"black\", label='Data points')\n",
        "plt.plot(X_train,  regr.predict(X_train), color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "plt.plot(X_train, ransac.predict(X_train), color=\"red\", linewidth=2, label=r'RANSAC Regressor')\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Training data (zoom)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUHTocU5jyWz"
      },
      "source": [
        "Predict values with test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwUGd3NWEzUE"
      },
      "outputs": [],
      "source": [
        "line_y_ransac = ransac.predict(X_test)\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(X_test, y_test, color=\"black\", label='Data points')\n",
        "plt.plot(X_test, y_pred, color=\"blue\", linewidth=2, label=r'Linear Regression')\n",
        "plt.plot(X_test, line_y_ransac, color=\"red\", linewidth=2, label=r'RANSAC Regressor')\n",
        "\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('AveRooms')\n",
        "plt.ylabel('MedHouseVal')\n",
        "plt.ylim((0,5))\n",
        "plt.xlim((0,10))\n",
        "plt.title(\"Testing data\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqcArWDAOnrP"
      },
      "source": [
        "## Polynomial Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N372viHejyW0"
      },
      "source": [
        "Polynomial Regression assumes that the function to be fitted is a polynomial function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1Ciy7kpjyW0"
      },
      "source": [
        "This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial features of different degrees. We can see that a linear function (polynomial with degree 1) is not sufficient to fit the training samples. This is called **underfitting**, i.e., a model is too simplistic to capture the\n",
        "underlying patterns in the data. A polynomial of degree 4 approximates the true function almost perfectly. However, for higher degrees the model will **overfit** the training data, i.e., a model performs\n",
        "exceptionally well on the training data but fails to generalize to\n",
        "new, unseen data. We evaluate quantitatively **overfitting** / **underfitting** by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EcOLtMNjyW0"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snA33xW2OnrP"
      },
      "outputs": [],
      "source": [
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    linear_regression = linear_model.LinearRegression()\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
        "        degrees[i], -scores.mean(), scores.std()))\n",
        "\n",
        "# plt.savefig('./figures/polynomial_regresion.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FOH0uzROnrQ"
      },
      "source": [
        "## Polynomial Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8QdhH01jyW1"
      },
      "source": [
        "Ridge regression is a regularization technique that can be utilized by maintaining a loss function of the following form.\n",
        "\n",
        "$Loss = MSE + \\lambda ||w||^2_2$\n",
        "\n",
        "This leads to a parameter estimator equations,\n",
        "\n",
        "$\\hat{\\beta}_{ridge} = (X^T X + \\lambda I_p)^{-1}X^T y$\n",
        "\n",
        "It can be understood from the example below that ridge regression controls the overfitting problem, as the degree 15 polynomial fits the ground truth reasonably well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2VCvdd7OnrQ"
      },
      "outputs": [],
      "source": [
        "def true_fun(X):\n",
        "    return np.cos(1.5 * np.pi * X)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "n_samples = 30\n",
        "degrees = [1, 4, 15]\n",
        "alpha = 0.0001 # lambda in the slides\n",
        "\n",
        "X = np.sort(np.random.rand(n_samples))\n",
        "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
        "                                             include_bias=False)\n",
        "    ridge = linear_model.Ridge(alpha=alpha, fit_intercept=True)\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
        "                         (\"ridge_regresssion\", ridge)])\n",
        "    pipeline.fit(X[:, np.newaxis], y)\n",
        "\n",
        "    # Evaluate the models using crossvalidation\n",
        "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "\n",
        "    X_test = np.linspace(0, 1, 100)\n",
        "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
        "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
        "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
        "        degrees[i], -scores.mean(), scores.std()))\n",
        "\n",
        "# plt.savefig('./figures/polynomial_ridge_regresion.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YedZ8kMGOnrQ"
      },
      "source": [
        "## Ridge Coefficients as a Function of the Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDz0JC9kjyW1"
      },
      "source": [
        "This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, **a slight change in the target variable can cause huge variances in the calculated weights**. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise).\n",
        "\n",
        "When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmou9szoOnrQ"
      },
      "outputs": [],
      "source": [
        "# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n",
        "# License: BSD 3 clause\n",
        "\n",
        "\n",
        "# X is the 10x10 Hilbert matrix\n",
        "X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\n",
        "y = np.ones(10)\n",
        "\n",
        "# #############################################################################\n",
        "# Compute paths\n",
        "\n",
        "n_alphas = 200\n",
        "alphas = np.logspace(-10, -2, n_alphas)\n",
        "\n",
        "coefs = []\n",
        "for a in alphas:\n",
        "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
        "    ridge.fit(X, y)\n",
        "    coefs.append(ridge.coef_)\n",
        "\n",
        "# #############################################################################\n",
        "# Display results\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.plot(alphas, coefs)\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
        "plt.xlabel('$\\lambda$')\n",
        "plt.ylabel('weights')\n",
        "plt.title('Ridge coefficients as a function of the regularization')\n",
        "plt.axis('tight')\n",
        "# plt.savefig('./figures/redge_coeff_regularization.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS8j4V3LOnrR"
      },
      "source": [
        "## LASSO Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icjZpfwijyW1"
      },
      "source": [
        "Lasso regression is a regularization technique that can be utilized by maintaining a loss function of the following form.\n",
        "\n",
        "$Loss = MSE + \\lambda ||w||_1$\n",
        "\n",
        "Compare this with the ridge regression equation.\n",
        "\n",
        " Lasso regression is generally used in cases where it is preferable to have a sparse representation of the model. This is illustrated in the example below, where using Lasso regression, many coefficients are forced to be zero\n",
        " ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP65q-KjOnrR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import  Lasso\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate synthetic data with informative and irrelevant features\n",
        "X, y, true_coefficients = make_regression(\n",
        "    n_samples=100, n_features=50, n_informative=10, noise=20.5, coef=True, random_state=42\n",
        ")\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_model = Lasso(alpha=3)  # Alpha is the regularization strength\n",
        "\n",
        "# Fit the Lasso regression model to the data\n",
        "lasso_model.fit(X, y)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "simple_linear_model = linear_model.LinearRegression()\n",
        "\n",
        "# Fit the Linear Regression model to the data\n",
        "simple_linear_model.fit(X, y)\n",
        "\n",
        "# Plot the true coefficients and the estimated coefficients for Lasso and Linear Regression\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(true_coefficients, label='True Coefficients', marker='o')\n",
        "plt.plot(lasso_model.coef_, label='Lasso Regression', marker='x')\n",
        "plt.plot(simple_linear_model.coef_, label='Linear Regression', marker='s')\n",
        "\n",
        "plt.xlabel('Coefficient Index')\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('True vs. Estimated Coefficients')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of non-zero coefficients (Linear Regression):\", np.sum(simple_linear_model.coef_ != 0))\n",
        "print(\"Number of non-zero coefficients (Lasso Regression):\", np.sum(lasso_model.coef_ != 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNDWwfOwOnrR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the true coefficients as stem plot\n",
        "plt.stem(range(len(true_coefficients)), true_coefficients, markerfmt='go', linefmt='g', basefmt=' ')\n",
        "\n",
        "# Plot the Lasso coefficients as stem plot with a small offset\n",
        "plt.stem(np.arange(len(true_coefficients)) + 0.3, lasso_model.coef_, markerfmt='rx', linefmt='r', basefmt=' ')\n",
        "\n",
        "# Plot the Linear Regression coefficients as stem plot with a larger offset\n",
        "plt.stem(np.arange(len(true_coefficients)) + 0.5, simple_linear_model.coef_, markerfmt='bs', linefmt='b', basefmt=' ')\n",
        "\n",
        "plt.xlabel('Coefficient Index')\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('True vs. Estimated Coefficients')\n",
        "plt.legend(['True Coefficients', 'Lasso Regression', 'Linear Regression'])\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geSMXVsXOnrS"
      },
      "source": [
        "## Multi Variable Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8io1O45jyW2"
      },
      "source": [
        "Review the code given below on multivariable linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iaQXkG8OnrS"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load the diabetes dataset\n",
        "data_X, data_y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "#remember index starts from 0\n",
        "# Split the data into training/testing sets\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(data_X, data_y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val  = model_selection.train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
        "\n",
        "print(\"Total dataset elements\",data_X.shape)\n",
        "print(\"Train dataset elements\",X_train.shape)\n",
        "print(\"Validatiom dataset elements\",X_val.shape)\n",
        "print(\"Test dataset elements\",X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dvQ8MpsjyW2"
      },
      "source": [
        "Using sklearn to fit the parameters and find the regression coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx2VOvH_OnrT"
      },
      "outputs": [],
      "source": [
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "# The intercept\n",
        "print(\"Coefficients: \\n\", regr.intercept_)\n",
        "\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n",
        "\n",
        "#see the impact of each variable\n",
        "\n",
        "regr_coef=regr.coef_/np.max(regr.coef_)\n",
        "print(\"Coefficients: \\n\", regr_coef)\n",
        "#Here, value=1 is the most relevant feature.\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "california_housing.data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYiRt2JmjyW2"
      },
      "source": [
        "Calculating p values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyE4wRHCOnrT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming you have already fitted the linear regression model (regr.fit(X_train, y_train))\n",
        "\n",
        "# Add a constant column to the feature matrix (required by statsmodels)\n",
        "X_train_with_constant = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the OLS (Ordinary Least Squares) model\n",
        "ols_model = sm.OLS(y_train, X_train_with_constant).fit()\n",
        "\n",
        "# Get summary statistics of the model\n",
        "summary = ols_model.summary()\n",
        "\n",
        "# Extract p-values from the summary for all features\n",
        "p_values = summary.tables[1].data[1:]\n",
        "\n",
        "# Create a DataFrame to associate p-values with feature names\n",
        "p_values_df = pd.DataFrame(p_values, columns=['Feature', 'Coefficient', 'Standard Error', 't-value', 'P-Value', 'Lower CI', 'Upper CI'])\n",
        "p_values_df['P-Value'] = p_values_df['P-Value'].astype(float)\n",
        "\n",
        "\n",
        "print(summary)\n",
        "print(p_values_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6sF-c0UjyW2"
      },
      "source": [
        "Calculating Standard errors and t-values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MI4gFQ0OnrT"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "lm = linear_model.LinearRegression()\n",
        "lm.fit(X_train,y_train)\n",
        "params = np.append(lm.intercept_,lm.coef_)\n",
        "predictions = lm.predict(X_train)\n",
        "\n",
        "newX = pd.DataFrame({\"Constant\":np.ones(len(X_train))}).join(pd.DataFrame(X_train))\n",
        "MSE = (sum((y_train-predictions)**2))/(len(newX)-len(newX.columns))\n",
        "\n",
        "# Note if you don't want to use a DataFrame replace the two lines above with\n",
        "# newX = np.append(np.ones((len(X),1)), X, axis=1)\n",
        "# MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))\n",
        "\n",
        "var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
        "sd_b = np.sqrt(var_b)\n",
        "ts_b = params/ sd_b\n",
        "\n",
        "\n",
        "\n",
        "sd_b = np.round(sd_b,3)\n",
        "ts_b = np.round(ts_b,3)\n",
        "\n",
        "params = np.round(params,4)\n",
        "\n",
        "myDF3 = pd.DataFrame()\n",
        "myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"] = [params,sd_b,ts_b]\n",
        "print(myDF3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}